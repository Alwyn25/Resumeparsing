{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCty5unn0myj"
   },
   "source": [
    "## NER Training Preparation\n",
    "- Download base `config` file from https://spacy.io/usage/training#quickstart\n",
    "- Modify config file for training data\n",
    "- Prepare training data\n",
    "- Do training\n",
    "- Final testing\n",
    "\n",
    "Data\n",
    "- https://github.com/Alwyn25/Resumeparsing.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oI0rh9uEsHCk"
   },
   "source": [
    "## Data Annotation Tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_fAyuO4sKsg"
   },
   "source": [
    "- http://agateteam.org/spacynerannotate/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JqnVd_-l67Kg",
    "outputId": "9c01983f-8a9c-436c-93f0-bdc2840ac0b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 0: fg: no job control\n",
      "Requirement already satisfied: spacy in /home/alwyn-python/Udemy Courses/OCR-NLP/docuscanenv/lib/python3.8/site-packages (2.3.9)\n",
      "Collecting spacy\n",
      "  Using cached spacy-3.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/alwyn-python/Udemy Courses/OCR-NLP/docuscanenv/lib/python3.8/site-packages (from spacy) (1.10.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/alwyn-python/Udemy Courses/OCR-NLP/docuscanenv/lib/python3.8/site-packages (from spacy) (2.28.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/alwyn-python/Udemy Courses/OCR-NLP/docuscanenv/lib/python3.8/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/alwyn-python/Udemy Courses/OCR-NLP/docuscanenv/lib/python3.8/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: jinja2 in /home/alwyn-python/Udemy Courses/OCR-NLP/docuscanenv/lib/python3.8/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/alwyn-python/Udemy Courses/OCR-NLP/docuscanenv/lib/python3.8/site-packages (from spacy) (4.64.1)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Using cached catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting thinc<8.2.0,>=8.1.0\n",
      "  Using cached thinc-8.1.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (828 kB)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/alwyn-python/Udemy Courses/OCR-NLP/docuscanenv/lib/python3.8/site-packages (from spacy) (6.3.0)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Using cached srsly-2.4.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (492 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/alwyn-python/Udemy Courses/OCR-NLP/docuscanenv/lib/python3.8/site-packages (from spacy) (1.24.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/alwyn-python/Udemy Courses/OCR-NLP/docuscanenv/lib/python3.8/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/alwyn-python/Udemy Courses/OCR-NLP/docuscanenv/lib/python3.8/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/alwyn-python/Udemy Courses/OCR-NLP/docuscanenv/lib/python3.8/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/alwyn-python/Udemy Courses/OCR-NLP/docuscanenv/lib/python3.8/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/alwyn-python/Udemy Courses/OCR-NLP/docuscanenv/lib/python3.8/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/alwyn-python/Udemy Courses/OCR-NLP/docuscanenv/lib/python3.8/site-packages (from spacy) (3.0.11)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/alwyn-python/Udemy Courses/OCR-NLP/docuscanenv/lib/python3.8/site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: setuptools in /home/alwyn-python/Udemy Courses/OCR-NLP/docuscanenv/lib/python3.8/site-packages (from spacy) (66.0.0)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/alwyn-python/Udemy Courses/OCR-NLP/docuscanenv/lib/python3.8/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/alwyn-python/Udemy Courses/OCR-NLP/docuscanenv/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/alwyn-python/Udemy Courses/OCR-NLP/docuscanenv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/alwyn-python/Udemy Courses/OCR-NLP/docuscanenv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/alwyn-python/Udemy Courses/OCR-NLP/docuscanenv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/alwyn-python/Udemy Courses/OCR-NLP/docuscanenv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Using cached blis-0.7.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/alwyn-python/Udemy Courses/OCR-NLP/docuscanenv/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/alwyn-python/Udemy Courses/OCR-NLP/docuscanenv/lib/python3.8/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/alwyn-python/Udemy Courses/OCR-NLP/docuscanenv/lib/python3.8/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Installing collected packages: catalogue, blis, srsly, thinc, spacy\n",
      "  Attempting uninstall: catalogue\n",
      "    Found existing installation: catalogue 1.0.2\n",
      "    Uninstalling catalogue-1.0.2:\n",
      "      Successfully uninstalled catalogue-1.0.2\n",
      "  Attempting uninstall: blis\n",
      "    Found existing installation: blis 0.4.1\n",
      "    Uninstalling blis-0.4.1:\n",
      "      Successfully uninstalled blis-0.4.1\n",
      "  Attempting uninstall: srsly\n",
      "    Found existing installation: srsly 1.0.6\n",
      "    Uninstalling srsly-1.0.6:\n",
      "      Successfully uninstalled srsly-1.0.6\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 7.4.6\n",
      "    Uninstalling thinc-7.4.6:\n",
      "      Successfully uninstalled thinc-7.4.6\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 2.3.9\n",
      "    Uninstalling spacy-2.3.9:\n",
      "      Successfully uninstalled spacy-2.3.9\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spacy-transformers 0.6.2 requires spacy<2.4.0,>=2.3.0, but you have spacy 3.5.0 which is incompatible.\n",
      "spacy-transformers 0.6.2 requires srsly<1.1.0,>=0.0.7, but you have srsly 2.4.5 which is incompatible.\n",
      "en-core-web-sm 2.3.1 requires spacy<2.4.0,>=2.3.0, but you have spacy 3.5.0 which is incompatible.\n",
      "en-core-web-lg 2.3.1 requires spacy<2.4.0,>=2.3.0, but you have spacy 3.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed blis-0.7.9 catalogue-2.0.8 spacy-3.5.0 srsly-2.4.5 thinc-8.1.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!%pip install spacy_transformers\n",
    "%pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PyLWQZ6E8g0N"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "rd3B7BZP9XXl",
    "outputId": "492e3c3a-5dd0-45b8-99fa-0acaa9cc09d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NSbS1WXy9ZsE",
    "outputId": "d47d94e6-55fd-42c8-aeb3-4b557654061e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: nvidia-smi: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BuEiebek9dgk",
    "outputId": "66121630-98c9-4cbf-e14b-da1367adfe7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Resumeparsing'...\n",
      "remote: Enumerating objects: 27, done.\u001b[K\n",
      "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
      "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
      "remote: Total 27 (delta 1), reused 0 (delta 0), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (27/27), 5.30 MiB | 1.37 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Alwyn25/Resumeparsing.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "oGS4tlrN9ucn"
   },
   "outputs": [],
   "source": [
    "cv_data = json.load(open('content/Resumeparsing/data/training/train_data.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1G9A-pfm9uZX",
    "outputId": "cdffafe2-1518-4d62-832b-923606241530"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vmom1SQ69uWD",
    "outputId": "9ea59579-9607-451d-f03d-e39fcc1fa9d8"
   },
   "outputs": [],
   "source": [
    "!python -m spacy init fill-config /content/Resumeparsing/data/training/base_config.cfg /content/Resumeparsing/data/training/config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "eqTzlCy_9uPI"
   },
   "outputs": [],
   "source": [
    "# cv_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "WESHLY1-_F6C"
   },
   "outputs": [],
   "source": [
    "def get_spacy_doc(file, data):\n",
    "    nlp = spacy.blank('en')\n",
    "    db = DocBin()\n",
    "\n",
    "    for text, annot in tqdm(data):\n",
    "        doc = nlp.make_doc(text)\n",
    "        annot = annot['entities']\n",
    "\n",
    "        ents = []\n",
    "        entity_indices = []\n",
    "\n",
    "        for start, end, label in annot:\n",
    "            skip_entity = False\n",
    "            for idx in range(start, end):\n",
    "                if idx in entity_indices:\n",
    "                    skip_entity=True\n",
    "                    break\n",
    "            if skip_entity==True:\n",
    "                continue\n",
    "\n",
    "            entity_indices = entity_indices + list(range(start, end))\n",
    "\n",
    "            try:\n",
    "                span = doc.char_span(start, end, label=label, alignment_mode='strict')\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            if span is None:\n",
    "                err_data = str([start, end]) + \"    \" + str(text) + \"\\n\"\n",
    "                file.write(err_data)\n",
    "\n",
    "            else:\n",
    "                ents.append(span)\n",
    "\n",
    "        try:\n",
    "            doc.ents = ents\n",
    "            db.add(doc)\n",
    "        except:\n",
    "              pass\n",
    "\n",
    "    return db\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "EQvo3pLG_Fl6"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(cv_data, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bUyOHfM3_FgF",
    "outputId": "262c8b69-6a6f-4877-f13b-cb737c9980a5",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140, 60)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wxbTgHZA_FdP",
    "outputId": "1d086448-eee9-4c62-cc06-3ab44f8f7715"
   },
   "outputs": [],
   "source": [
    "file = open('error.txt', 'w')\n",
    "\n",
    "db = get_spacy_doc(file, train)\n",
    "db.to_disk('train_data.spacy')\n",
    "\n",
    "db = get_spacy_doc(file, test)\n",
    "db.to_disk('test_data.spacy')\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Lb0Qzol_FaX",
    "outputId": "ccc2738e-ebf4-403c-94dd-894addd8e098"
   },
   "outputs": [],
   "source": [
    "len(db.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mQeUk2FlRdJE",
    "outputId": "f88d7847-95d2-4919-8b07-9bd59f2ca7c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2022-06-17 20:10:05,414] [INFO] Set up nlp object from config\n",
      "[2022-06-17 20:10:05,424] [INFO] Pipeline: ['transformer', 'ner']\n",
      "[2022-06-17 20:10:05,428] [INFO] Created vocabulary\n",
      "[2022-06-17 20:10:05,429] [INFO] Finished initializing nlp object\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[2022-06-17 20:10:21,837] [INFO] Initialized pipeline components: ['transformer', 'ner']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  -------------  --------  ------  ------  ------  ------\n",
      "  0       0        2326.78   1601.33    0.20    0.10    4.29    0.00\n",
      "  4     200       87096.18  74308.07   10.98    8.76   14.71    0.11\n",
      "  8     400        8253.39  30830.46   27.41   20.90   39.83    0.27\n",
      " 12     600        7999.91  28268.78   33.93   23.63   60.17    0.34\n",
      " 16     800       22138.59  27563.00   31.65   21.00   64.22    0.32\n",
      " 20    1000        4011.86  24902.68   33.33   22.95   60.91    0.33\n",
      "\n",
      "Aborted!\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train /content/Resumeparsing/data/training/config.cfg --output ./output --paths.train ./train_data.spacy --paths.dev ./test_data.spacy --gpu-id 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQSZj0Y2eOqQ"
   },
   "source": [
    "### Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fv-i4DBj_FXh"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('/content/output/model-best')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WKdBGjvP_FUo",
    "outputId": "33695240-8017-4610-a227-1720f8bc1e7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laxmi Kant    ->>>>>  Name\n",
      "Microsoft    ->>>>>  Companies worked at\n",
      "10 years    ->>>>>  Years of Experience\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('my name is Laxmi Kant Tiwari. I worked at Microsoft. I have 10 years of experience')\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"   ->>>>> \", ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vUDJOY03_FRs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting PyMuPDF\n",
      "  Using cached PyMuPDF-1.21.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
      "Installing collected packages: PyMuPDF\n",
      "Successfully installed PyMuPDF-1.21.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X--FwjAI_FOr"
   },
   "outputs": [],
   "source": [
    "import sys, fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YbOdPY-c_FL2"
   },
   "outputs": [],
   "source": [
    "fname = '/content/Resumeparsing/data/test/Alice Clark CV.pdf'\n",
    "doc = fitz.open(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XqPBztBf_FI7"
   },
   "outputs": [],
   "source": [
    "# doc = [page.getText() for page in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CDYV5NGB_FGK"
   },
   "outputs": [],
   "source": [
    "text = \" \"\n",
    "for page in doc:\n",
    "    text = text + str(page.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m70UXjnF_FDW"
   },
   "outputs": [],
   "source": [
    "text = text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HrvQWeZt_FAw"
   },
   "outputs": [],
   "source": [
    "text = ' '.join(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "id": "BXGAIRcE_E9t",
    "outputId": "bfbb4c30-c689-4c56-8eb0-1a2e020055e7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Alice Clark AI / Machine Learning Delhi, India Email me on Indeed • 20+ years of experience in data handling, design, and development • Data Warehouse: Data analysis, star/snow flake scema data modelling and design specific to data warehousing and business intelligence • Database: Experience in database designing, scalability, back-up and recovery, writing and optimizing SQL code and Stored Procedures, creating functions, views, triggers and indexes. Cloud platform: Worked on Microsoft Azure cloud services like Document DB, SQL Azure, Stream Analytics, Event hub, Power BI, Web Job, Web App, Power BI, Azure data lake analytics(U-SQL) Willing to relocate anywhere WORK EXPERIENCE Software Engineer Microsoft – Bangalore, Karnataka January 2000 to Present 1. Microsoft Rewards Live dashboards: Description: - Microsoft rewards is loyalty program that rewards Users for browsing and shopping online. Microsoft Rewards members can earn points when searching with Bing, browsing with Microsoft Edge and making purchases at the Xbox Store, the Windows Store and the Microsoft Store. Plus, user can pick up bonus points for taking daily quizzes and tours on the Microsoft rewards website. Rewards live dashboards gives a live picture of usage world-wide and by markets like US, Canada, Australia, new user registration count, top/bottom performing rewards offers, orders stats and weekly trends of user activities, orders and new user registrations. the PBI tiles gets refreshed in different frequencies starting from 5 seconds to 30 minutes. Technology/Tools used EDUCATION Indian Institute of Technology – Mumbai 2001 SKILLS Machine Learning, Natural Language Processing, and Big Data Handling ADDITIONAL INFORMATION Professional Skills • Excellent analytical, problem solving, communication, knowledge transfer and interpersonal skills with ability to interact with individuals at all the levels • Quick learner and maintains cordial relationship with project manager and team members and good performer both in team and independent job environments • Positive attitude towards superiors &amp; peers • Supervised junior developers throughout project lifecycle and provided technical assistance'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MnyMRYFlg3PY",
    "outputId": "ff0e72f4-3db2-44e9-df98-6f6e3cc8cb9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice Clark    ->>>>>  Name\n",
      "AI / Machine Learning    ->>>>>  Degree\n",
      "Delhi    ->>>>>  Location\n",
      "• 20    ->>>>>  Degree\n",
      "• Data    ->>>>>  Degree\n",
      "• Database    ->>>>>  Degree\n",
      "Microsoft    ->>>>>  Companies worked at\n",
      "Software Engineer    ->>>>>  Designation\n",
      "Microsoft    ->>>>>  Companies worked at\n",
      "– Bangalore,    ->>>>>  Degree\n",
      "Microsoft    ->>>>>  Companies worked at\n",
      "Microsoft    ->>>>>  Companies worked at\n",
      "Microsoft    ->>>>>  Companies worked at\n",
      "Microsoft    ->>>>>  Companies worked at\n",
      "Microsoft    ->>>>>  Companies worked at\n",
      "Microsoft    ->>>>>  Companies worked at\n",
      "Indian Institute of Technology    ->>>>>  College Name\n",
      "– Mumbai    ->>>>>  Degree\n",
      "Machine Learning, Natural Language Processing, and Big Data Handling ADDITIONAL INFORMATION Professional Skills • Excellent analytical, problem solving, communication, knowledge transfer and interpersonal skills with ability to interact with individuals at all the levels • Quick learner and maintains cordial relationship with project manager and team members and good performer both in team and independent job environments • Positive attitude towards superiors &amp; peers • Supervised junior developers throughout project lifecycle and provided technical assistance    ->>>>>  Skills\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"   ->>>>> \", ent.label_)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Resume (CV) Parsing.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
